---
title: "Internel_data_cleansing"
author: "Michiel van Eldik"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
---

# __0. Prologue__
```{r message=FALSE}
library(dplyr)
library(kableExtra)
library(car)
library(tidytext)
library(lubridate)
library(tm)
library(stringr)
library(NLP)
library(SnowballC)
```

```{r Load data}
input <- read.csv("sao_paulo_state_dataset.csv")
```

``` {r create variable}
geo_sao_paolo_df <- input
```

# __1. Text data pre-processing__
## __1.0. what we have & what are our goals__

__We have:__

* Portuguese;
* Short phrases, in many cases just single words, which can limit 
the utility of certain approaches;
* Context is already known (Indicated by quantitative score), 
which can be used to our advantage;

__What we want:__

* Be able to distinguish between _product-related_ and _freight-related_ issues.
* Gain an understanding on characteristics of reviews, such as...
  + Length
  + Lexicon
  + etc. 

## __1.0. Data type__
Currently, the comments and titles are a factor data type. 
The general preference for text analysis is character string representation. 
In some cases it's even required, so let's go ahead and change that!
``` {r }
geo_sao_paolo_df <- geo_sao_paolo_df %>%
  mutate(
    review_comment_message = as.character(review_comment_message),
    review_comment_title = as.character(review_comment_title)
        )
```

## __1.1. Standardization__

* lowercase
* Change encoding to English-letters only (ASCII)

``` {r standardization}
geo_sao_paolo_df <- geo_sao_paolo_df %>%
  mutate(
    review_comment_message = tolower(
                                iconv(
                                  review_comment_message, 
                                  to = "ASCII//TRANSLIT"
                                     )
                                    ),
    review_comment_title = tolower(
                              iconv(
                                review_comment_title,
                                to = "ASCII//TRANSLIT"
                                   )
                                  )
        )
```

## __Tidytext__

https://en.wikipedia.org/wiki/Portuguese_orthography

Portuguese language rarely uses double letters apart from:

* ss
* rr



### Bunch of cleaning

``` {r }
geo_sao_paolo_df  <- geo_sao_paolo_df %>%
  mutate(
    # Remove punctuation 
    review_comment_message = gsub("[[:punct:]]+"," ", review_comment_message),
    # Remove digits
    review_comment_message = gsub("[[:digit:]]+"," ", review_comment_message),
    # remove double characters except ss and rr
    review_comment_message = gsub("([a-q t-z])\\1+", "\\1", 
                                       review_comment_message, 
                                       perl = TRUE),
    # Remove these next line \n things
    review_comment_message = str_remove_all(
      review_comment_message, "\\n"),
    # Between words white spaces
    review_comment_message = str_replace_all(
      review_comment_message, "\\s+", " "),
    # Trailing white spaces
    review_comment_message = str_replace_all(
      review_comment_message, "\\s*$", ""),
    # leading white spaces
    review_comment_message = str_replace_all(
      review_comment_message, "^\\s*", "")
         )
```


### Remove extremely short words 

``` {r }
geo_sao_paolo_df <- geo_sao_paolo_df %>%
  mutate(
    
    review_comment_message = ifelse(
      nchar(review_comment_message) == 1,
      "",
      review_comment_message),
    
    review_comment_message = ifelse(
      nchar(review_comment_message) == 2 & review_comment_message != "ok",
      "",
      review_comment_message)
        )
```

# __Tokenization__

``` {r }
tokenized <- str_split(geo_sao_paolo_df$review_comment_message, " ")
```


# __Stemming vs lemmatization__

* Lemmatization: More sophisticated, reqwuires more effort and knowledge
* Stemming: More straightforward, not as sophisticated.

Stemming is more appropriate. 

``` {r }
to_stem <- tokenized[[1]]
to_stem
stemDocument(to_stem, language = "pt")
```

``` {r }
stem <- wordStem(to_stem, language = "portuguese")
```


``` {r }
full_char <- paste(stem, collapse = " ")
full_char
```



``` {r }

```



```{r }
receb <- geo_sao_paolo_df %>% 
  filter(grepl('receb', review_comment_message))
```


# __Lemmatization__





# __cool packages__

https://hub.packtpub.com/9-useful-r-packages-for-nlp-text-mining/

``` {r}
library(languageR) # lexical stuff cool

```

# Frequencies

``` {r}
text <- as.character(geo_sao_paolo_df$review_comment_message)

text_df <- tibble(line = 1:41638, text = text)


new_text_df <- text_df %>%
  unnest_tokens(word, text)

new_text_df %>%
  count(word, sort = TRUE)
```














(stemming and tokenization first!)
(Use context, so do this after knowledge of late, early, on time)
``` {r }

stop_words <- c(
  "e", "o", "a", "do", "de", "que", "no", "um", 
)

freight_words <- c(
  "chegou", "recebi", "entrega", "prazo"
)

```
## 1.2. Punctuation
I do want to keep track of these. It's interesting to know about

* Question marks
* Exclamation marks



## 1.3. Numbers


## 1.4. Misc weird stuff

## 1.4. Tokenization 

## 1.5. Stop words


Use Portuguese stopwords library

## 1.6. Lemmatization 





## explore text data
Basically I want an answer to whether which comments are noise and which
ones can be taken seriously.

### words with one or multiple expression signs (?, !, *, ...)

``` {r }
# is there a dot?
try <- geo_sao_paolo_df[grep(".", geo_sao_paolo_df$review_comment_message),]

try <- geo_sao_paolo_df[grep("!", geo_sao_paolo_df$review_comment_message),]

```

### Comments with numbers 

### Words with no meaning ()

### single words

### short comments vs actual stories that are useful 
length > xxx

# Dates data 
``` {r }
geo_sao_paolo_df <- geo_sao_paolo_df %>%
  mutate(
    review_creation_date = as.Date(review_creation_date,
                                   format = "%Y-%m-%d %H:%M:%S"),
    review_answer_timestamp = as.Date(review_answer_timestamp,
                                   format = "%Y-%m-%d %H:%M:%S"),
    order_purchase_timestamp = as.Date(order_purchase_timestamp, 
                                   format = "%Y-%m-%d %H:%M:%S"),
    order_approved_at = as.Date(order_approved_at,
                                   format = "%Y-%m-%d %H:%M:%S"),
    order_delivered_carrier_date = as.Date(order_delivered_carrier_date,
                                   format = "%Y-%m-%d %H:%M:%S"),
    order_delivered_customer_date = as.Date(order_delivered_customer_date,
                                   format = "%Y-%m-%d %H:%M:%S"),
    order_estimated_delivery_date = as.Date(order_estimated_delivery_date,
                                   format = "%Y-%m-%d %H:%M:%S")
         )
```

time relative variables
``` {r }
geo_sao_paolo_df <- geo_sao_paolo_df %>%
  mutate(
    diff_est_deliv = order_estimated_delivery_date - order_delivered_customer_date,
    diff_pur_est  = order_estimated_delivery_date - order_purchase_timestamp,
    diff_pur_deliv = order_delivered_customer_date - order_purchase_timestamp,
    diff_rev_crea_ans = review_creation_date - review_answer_timestamp,
    diff_rev_est_ans = order_estimated_delivery_date - review_answer_timestamp,
    diff_rev_deliv_ans = order_delivered_customer_date - review_answer_timestamp
         )
```

weekend dummies 
``` {r }
geo_sao_paolo_df <- geo_sao_paolo_df %>%
  mutate(
    # show weekday of a date
    review_sent_dow = wday(review_creation_date, label = TRUE),
    review_answer_dow = wday(review_answer_timestamp, label = TRUE),
    
    # Dummy variable weekend yes/no
    review_sent_wknd = ifelse(
      review_sent_dow == 'zo' | review_sent_dow == 'za', 1, 0),
    review_answer_wknd = ifelse(
      review_answer_dow == 'zo' | review_answer_dow == 'za', 1, 0)
         )
```

Year dummies 
``` {r }
geo_sao_paolo_df<- geo_sao_paolo_df %>%
  mutate(
    y_2016 = ifelse(year(order_purchase_timestamp) == '2016', 1, 0),
    y_2017 = ifelse(year(order_purchase_timestamp) == '2017', 1, 0),
    y_2018 = ifelse(year(order_purchase_timestamp) == '2016', 1, 0)
         )
```

# Duplicates in customer cities



# Past behaviour of customer

* previous recensy 
* frequency
* new_yes/no






