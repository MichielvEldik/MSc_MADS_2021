---
title: "Internel_data_cleansing"
author: "Michiel van Eldik"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
---

# __0. Prologue__
```{r message=FALSE}
library(dplyr)
library(kableExtra)
library(car)
library(tidytext)
library(lubridate)
library(tm)
library(stringr)
library(NLP)
library(SnowballC)
library(koRpus)
library(koRpus.lang.pt)

```

```{r Load data}
input <- read.csv("sao_paulo_state_dataset.csv")
```

``` {r create variable}
geo_sao_paolo_df <- input
sanity_geo_sao_paulo <- input
```

# __1. Text data pre-processing__
## __1.0. what we have & what are our goals__

__We have:__

* Portuguese;
* Short phrases, in many cases just single words, which can limit 
the utility of certain approaches;
* Context is already known (Indicated by quantitative score), 
which can be used to our advantage;

__What we want:__

* Be able to distinguish between _product-related_ and _freight-related_ issues.
* Gain an understanding on characteristics of reviews, such as...
  + Length
  + Lexicon
  + etc. 

## __1.1. Data type__
Currently, the comments and titles are a factor data type. 
The general preference for text analysis is character string representation. 
In some cases it's even required, so let's go ahead and change that!
``` {r }
geo_sao_paolo_df <- geo_sao_paolo_df %>%
  mutate(
    review_comment_message = as.character(review_comment_message),
    review_comment_title = as.character(review_comment_title)
        )
```

## __1.2. Remove numbers, punctuations, doubles__

``` {r }
geo_sao_paolo_df  <- geo_sao_paolo_df %>%
  mutate(
    # Remove punctuation 
    review_comment_message = gsub("[[:punct:]]+"," ", review_comment_message),
    # Remove digits
    review_comment_message = gsub("[[:digit:]]+"," ", review_comment_message),
    # remove double characters except ss and rr
    review_comment_message = gsub("([a-q t-z])\\1+", "\\1", 
                                       review_comment_message, 
                                       perl = TRUE),
    # Get rid of line break strings
    review_comment_message = gsub("\r?\n|\r", " ", review_comment_message)
  )

# Same for title
geo_sao_paolo_df  <- geo_sao_paolo_df %>%
  mutate(
    # Remove punctuation 
    review_comment_title = gsub("[[:punct:]]+"," ", review_comment_title),
    # Remove digits
    review_comment_title = gsub("[[:digit:]]+"," ", review_comment_title),
    # remove double characters except ss and rr
    review_comment_title = gsub("([a-q t-z])\\1+", "\\1", 
                                       review_comment_title, 
                                       perl = TRUE),
    # Get rid of line break strings
    review_comment_title = gsub("\r?\n|\r", " ", review_comment_title)
  )
```

## __1.3. Remove extremely short words__ 

``` {r }
geo_sao_paolo_df <- geo_sao_paolo_df %>%
  mutate(
    
    review_comment_message = ifelse(
      nchar(review_comment_message) == 1,
      "",
      review_comment_message),
    
    review_comment_message = ifelse(
      nchar(review_comment_message) == 2 & review_comment_message != "ok",
      "",
      review_comment_message)
        )
```


## __1.4. To lower case__

``` {r }
geo_sao_paolo_df <- geo_sao_paolo_df %>%
  mutate(
    review_comment_message = tolower(review_comment_message),
    review_comment_title = tolower(review_comment_title)
        )
```


## __1.5. Lemmatization__
### __1.5.0. Lemmatization vs. stemming__
The choice was made to use lemmatization over stemming. 

* Compared to English the morphology of Portuguese is more complex, which 
runs the risk of over- and understemming.
* Although lemmatization is more computationally complex, I only need to 
go through it once as this is a one-shot research project.
* Lemmatization generally provides better results, espically in inflection-rich
languages like Spanish or Portuguese.

Lemmatization will be done in __Python__ using _spaCy__ because:

* Options to do it _easily_ (in my eyes) in R are limited. In __spaCy__, 
I don't even have to turn it into a corpus.
* the __spaCy__ library offers trained models and pipelines that have exhibited
above average performance in [recent tests for the Portuguese language.](https://lars76.github.io/2018/05/08/portuguese-lemmatizers.html)\n


First, export current data - with only the necessary columns - to csv.
This csv will be used in the Python script.
``` {r eval = FALSE}
to_write <- geo_sao_paolo_df %>%
  select(review_id, review_comment_message)
write.csv(to_write, "for_spacy.csv", row.names = FALSE)
```

The lemmatization in Python.

``` {python eval = FALSE}
import spacy
import pandas as pd
import math

# Load portuguese pipeline
nlp = spacy.load("pt_core_news_sm")

# Loaad data that I exported from the current Rmarkdown file
df = pd.read_csv('/home/mitchy123/Documents/MSc_MADS_2021/for_spacy.csv')

# Assign new variable (not that functional, but helps when experimenting)
sub_df = df

# initialize empty list
lemma_listje = []

# Instruction of how data are seperated
# As we are using sentences, it is seperated with spaces
seperator = " "

# For loop that lemmatizes each word of each comment
for i in sub_df.iloc[:,1][sub_df.iloc[:,1].notna() == True]:
    i = nlp(i)
    interim_listje = []
    for word in i:
        interim_listje.append(word.lemma_)
    interim_listje = seperator.join(interim_listje)
    lemma_listje.append(interim_listje)

# Add indexes of rows where there is not NaN
nan_indices = sub_df.loc[pd.notna(sub_df["review_comment_message"]), :].index

# Create dataframe to export
d = {'sentence':lemma_listje, 'index':nan_indices}
new_df = pd.DataFrame(d)

# Export to csv, which will be used in this current Rmarkdown file again
new_df.to_csv('lemmatized.csv')
```

Loading and merging to the df we were working with.
```{r }
lemmatized <- read.csv('lemmatized.csv')

indiced_gsp <- geo_sao_paolo_df
indiced_gsp$index <- c(0: (nrow(indiced_gsp) - 1))

geo_sao_paolo_df <- merge(indiced_gsp, lemmatized, 
                  by.x = 'index',
                  by.y = 'index',
                  all.x = TRUE)

geo_sao_paolo_df <- geo_sao_paolo_df %>%
  mutate(review_comment_message = sentence) %>%
  select(
    - X,
    - sentence,
    - index
        )
```





``` {r }
indiced_gsp <- geo_sao_paolo_df
indiced_gsp$index <- c(0: (nrow(indiced_gsp) - 1))
```

``` {r }
geo_sao_paolo_df <- merge(indiced_gsp, lemmatized, 
                  by.x = 'index',
                  by.y = 'index',
                  all.x = TRUE)

geo_sao_paolo_df <- geo_sao_paolo_df %>%
  mutate(review_comment_message = sentence) %>%
  select(
    - X,
    - sentence
        )
```


# whitespaces hold it for a minute, maybe not needed. 

``` {r eval = FALSE}


    # Remove these next line \n things
    review_comment_message = str_remove_all(
      review_comment_message, "\\n"),
    # Between words white spaces
    review_comment_message = str_replace_all(
      review_comment_message, "\\s+", " ")#,
    # Trailing white spaces
    #review_comment_message = str_replace_all(
    #  review_comment_message, "\\s*$", ""),
    # leading white spaces
    #review_comment_message = str_replace_all(
    #  review_comment_message, "^\\s*", "")
         )

```


## __1.1. Standardization__

* lowercase
* Change encoding to English-letters only (ASCII)

``` {r eval = FALSE}
geo_sao_paolo_df <- geo_sao_paolo_df %>%
  mutate(
    review_comment_message = tolower(
                                iconv(
                                  review_comment_message, 
                                  to = "ASCII//TRANSLIT"
                                     )
                                    ),
    review_comment_title = tolower(
                              iconv(
                                review_comment_title,
                                to = "ASCII//TRANSLIT"
                                   )
                                  )
        )
```

# Frequencies

``` {r}
text <- as.character(geo_sao_paolo_df$review_comment_message)

text_df <- tibble(line = 1:41638, text = text)


new_text_df <- text_df %>%
  unnest_tokens(word, text)

new_text_df %>%
  count(word, sort = TRUE)
```

(stemming and tokenization first!)
(Use context, so do this after knowledge of late, early, on time)
``` {r }

stop_words <- c(
  "e", "o", "a", "do", "de", "que", "no", "um" 
)

freight_words <- c(
  "chegou", "recebi", "entrega", "prazo"
)

```




## explore text data
Basically I want an answer to whether which comments are noise and which
ones can be taken seriously.

### words with one or multiple expression signs (?, !, *, ...)

``` {r }
# is there a dot?
try <- geo_sao_paolo_df[grep(".", geo_sao_paolo_df$review_comment_message),]

try <- geo_sao_paolo_df[grep("!", geo_sao_paolo_df$review_comment_message),]

```


### Words with no meaning ()

### single words

### short comments vs actual stories that are useful 
length > xxx

# Dates data 
``` {r }
geo_sao_paolo_df <- geo_sao_paolo_df %>%
  mutate(
    review_creation_date = as.Date(review_creation_date,
                                   format = "%Y-%m-%d %H:%M:%S"),
    review_answer_timestamp = as.Date(review_answer_timestamp,
                                   format = "%Y-%m-%d %H:%M:%S"),
    order_purchase_timestamp = as.Date(order_purchase_timestamp, 
                                   format = "%Y-%m-%d %H:%M:%S"),
    order_approved_at = as.Date(order_approved_at,
                                   format = "%Y-%m-%d %H:%M:%S"),
    order_delivered_carrier_date = as.Date(order_delivered_carrier_date,
                                   format = "%Y-%m-%d %H:%M:%S"),
    order_delivered_customer_date = as.Date(order_delivered_customer_date,
                                   format = "%Y-%m-%d %H:%M:%S"),
    order_estimated_delivery_date = as.Date(order_estimated_delivery_date,
                                   format = "%Y-%m-%d %H:%M:%S")
         )
```

time relative variables
``` {r }
geo_sao_paolo_df <- geo_sao_paolo_df %>%
  mutate(
    diff_est_deliv = order_estimated_delivery_date - order_delivered_customer_date,
    diff_pur_est  = order_estimated_delivery_date - order_purchase_timestamp,
    diff_pur_deliv = order_delivered_customer_date - order_purchase_timestamp,
    diff_rev_crea_ans = review_creation_date - review_answer_timestamp,
    diff_rev_est_ans = order_estimated_delivery_date - review_answer_timestamp,
    diff_rev_deliv_ans = order_delivered_customer_date - review_answer_timestamp
         )
```

weekend dummies 
``` {r }
geo_sao_paolo_df <- geo_sao_paolo_df %>%
  mutate(
    # show weekday of a date
    review_sent_dow = wday(review_creation_date, label = TRUE),
    review_answer_dow = wday(review_answer_timestamp, label = TRUE),
    
    # Dummy variable weekend yes/no
    review_sent_wknd = ifelse(
      review_sent_dow == 'zo' | review_sent_dow == 'za', 1, 0),
    review_answer_wknd = ifelse(
      review_answer_dow == 'zo' | review_answer_dow == 'za', 1, 0)
         )
```

Year dummies 
``` {r }
geo_sao_paolo_df<- geo_sao_paolo_df %>%
  mutate(
    y_2016 = ifelse(year(order_purchase_timestamp) == '2016', 1, 0),
    y_2017 = ifelse(year(order_purchase_timestamp) == '2017', 1, 0),
    y_2018 = ifelse(year(order_purchase_timestamp) == '2016', 1, 0)
         )
```

# Duplicates in customer cities



# Past behaviour of customer

* previous recensy 
* frequency
* new_yes/no






