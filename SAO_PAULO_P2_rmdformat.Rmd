---
title: "Cleansing text data"
author: "Michiel van Eldik"
date: "`r Sys.Date()`"
output:
   rmdformats::downcute:
    self_contained: true
    thumbnails: true
    lightbox: true
    gallery: false
    highlight: tango
---


# __<a href="index.html">Back to main</a>__

<center> <a href="index.html">Back to main</a> </center>




# __1. Introduction__


```{r message=FALSE}
library(dplyr)
library(kableExtra)
library(car)
library(tidytext)
library(tm)
library(stringr)
library(NLP)
library(SnowballC)
library(koRpus)
library(koRpus.lang.pt)
library(qdap)
```

```{r Load data}
input <- read.csv("sao_paulo_state_dataset.csv")
```

``` {r create variable}
geo_sao_paolo_df <- input
sanity_geo_sao_paulo <- input
```


__We have:__

* Portuguese;
* Short phrases, in many cases just single words, which can limit 
the utility of certain approaches;
* Context is already known (Indicated by quantitative score), 
which can be used to our advantage;

__What we want:__

* Be able to distinguish between _product-related_ and _freight-related_ issues.
* Gain an understanding on characteristics of reviews, such as...
  + Length
  + Lexicon
  + etc. 

# __2. Convert data type__
Currently, the comments and titles are a factor data type. 
The general preference for text analysis is character string representation. 
In some cases it's even required, so let's go ahead and change that!
``` {r }
geo_sao_paolo_df <- geo_sao_paolo_df %>%
  mutate(
    review_comment_message = as.character(review_comment_message),
    review_comment_title = as.character(review_comment_title)
        )
```

# __3. Remove numbers, punctuation, doubles__ {3}
``` {r }
# For message
geo_sao_paolo_df  <- geo_sao_paolo_df %>%
  mutate(
    # Remove punctuation 
    review_comment_message = gsub("[[:punct:]]+"," ", review_comment_message),
    # Remove digits
    review_comment_message = gsub("[[:digit:]]+"," ", review_comment_message),
    # remove double characters except ss and rr
    review_comment_message = gsub("([a-q t-z])\\1+", "\\1", 
                                       review_comment_message, 
                                       perl = TRUE),
    # Get rid of line break strings
    review_comment_message = gsub("\r?\n|\r", " ", review_comment_message)
  )
```

``` {r }
# for title
geo_sao_paolo_df  <- geo_sao_paolo_df %>%
  mutate(
    # Remove punctuation 
    review_comment_title = gsub("[[:punct:]]+"," ", review_comment_title),
    # Remove digits
    review_comment_title = gsub("[[:digit:]]+"," ", review_comment_title),
    # remove double characters except ss and rr
    review_comment_title = gsub("([a-q t-z])\\1+", "\\1", 
                                       review_comment_title, 
                                       perl = TRUE),
    # Get rid of line break strings
    review_comment_title = gsub("\r?\n|\r", " ", review_comment_title)
  )
```

# __4. Remove extremely short comments__ 
``` {r }
geo_sao_paolo_df <- geo_sao_paolo_df %>%
  mutate(
    # comments with <2 don't add anything 
    review_comment_message = ifelse(
      nchar(review_comment_message) < 3 & review_comment_message != "ok",
      "",
      review_comment_message)
        )
```


# __5. To lower case__

``` {r }
geo_sao_paolo_df <- geo_sao_paolo_df %>%
  mutate(
    # convert comment message
    review_comment_message = tolower(review_comment_message),
    # convert title
    review_comment_title = tolower(review_comment_title)
        )
```


# __6. Record comment length__

I do this before lemmatization as lemmatization will change words.

``` {r }
geo_sao_paolo_df <- geo_sao_paolo_df %>%
  mutate(
    # total characters in the string
    bef_nchar = nchar(geo_sao_paolo_df$review_comment_message),
    # total number of separations (words) in the string 
    bef_nwords = lengths(strsplit(geo_sao_paolo_df$review_comment_message, " ")),
    # average number of letters per word 
    nchar_perword = nchar(geo_sao_paolo_df$review_comment_message) / lengths(strsplit(geo_sao_paolo_df$review_comment_message, " "))
  )
``` 

# __7. Lemmatization__
## __7.1. Lemmatization vs. stemming__
The choice was made to use lemmatization over stemming. 

* Compared to English the morphology of Portuguese is more complex, which 
runs the risk of over- and understemming.
* Although lemmatization is more computationally complex, I only need to 
go through it once as this is a one-shot research project.
* Lemmatization generally provides better results, especially in inflection-rich
languages like Spanish or Portuguese.

Lemmatization will be done in __Python__ using __spaCy__ because:

* Options to do it _easily_ (in my eyes) in R are limited. __spaCy__ is powerful
and requires only a few lines of code. 
* The __spaCy__ library offers trained models and pipelines that have exhibited
above average performance in [recent tests for the Portuguese language.](https://lars76.github.io/2018/05/08/portuguese-lemmatizers.html)\n

## __7.2. Write csv for export__
Although, yes, the `Reticulate` library allows for Python integration with 
Rmarkdown. I haven't managed to make it work yet so I do it in a separate 
Jupyter notebook script. So step 1 is writing our current data to a csv file
that can be openened in that script.

``` {r eval = FALSE}
to_write <- geo_sao_paolo_df %>%
  select(review_id, review_comment_message)
write.csv(to_write, "for_spacy.csv", row.names = FALSE)
```

## __7.3. Lemmatization with spaCy in Python__
__Discaimer:__ this code chunk is for display purposes only. 

``` {python eval = FALSE}
import spacy
import pandas as pd
import math

# Load portuguese pipeline
nlp = spacy.load("pt_core_news_sm")

# Loaad data that I exported from the current Rmarkdown file
df = pd.read_csv('/home/mitchy123/Documents/MSc_MADS_2021/for_spacy.csv')

# Assign new variable (not that functional, but helps when experimenting)
sub_df = df

# initialize empty list
lemma_listje = []

# Instruction of how data are seperated
# As we are using sentences, it is seperated with spaces
seperator = " "

# For loop that lemmatizes each word of each comment
for i in sub_df.iloc[:,1][sub_df.iloc[:,1].notna() == True]:
    i = nlp(i)
    interim_listje = []
    for word in i:
        interim_listje.append(word.lemma_)
    interim_listje = seperator.join(interim_listje)
    lemma_listje.append(interim_listje)

# Add indexes of rows where there is not NaN
nan_indices = sub_df.loc[pd.notna(sub_df["review_comment_message"]), :].index

# Create dataframe to export
d = {'sentence':lemma_listje, 'index':nan_indices}
new_df = pd.DataFrame(d)

# Export to csv, which will be used in this current Rmarkdown file again
new_df.to_csv('lemmatized.csv')
```

## __7.4. Reading and merging lemmatized reviews__
The lemmatized data needs to be brought back to this Rmarkdown.

```{r }
# Load data
lemmatized <- read.csv('lemmatized.csv')

# Create index column that will help with merge
indiced_gsp <- geo_sao_paolo_df
indiced_gsp$index <- c(0: (nrow(indiced_gsp) - 1))

# Merge by index
geo_sao_paolo_df <- merge(indiced_gsp, lemmatized, 
                  by.x = 'index',
                  by.y = 'index',
                  all.x = TRUE)

# Get rid of unnecessary columns
geo_sao_paolo_df <- geo_sao_paolo_df %>%
  mutate(review_comment_message = sentence) %>%
  select(
    - X,
    - sentence,
    - index
        )

# Data need to be turned to string type from factor again
geo_sao_paolo_df <- geo_sao_paolo_df %>%
  mutate(
    review_comment_message = as.character(review_comment_message),
    review_comment_title = as.character(review_comment_title)
        )
```

## __7.5. Example case__
I have done more rigorous sanity checks behind the scenes to the extent that my
very limited knowledge of the Portuguese language allows me. An example is shown
below.\n

__Before:__
``` {r }
# Nonlemmatized dataset
print(as.character(sanity_geo_sao_paulo$review_comment_message[8]))
```
__After:__
```{r }

# Lemmatized dataset
print(geo_sao_paolo_df$review_comment_message[8])
```
What can be seen:

* _Entrega_ is the present tense conjugation of lemma _entregar_;
* _Chegou_ is the past perfect conjugation of the lemma of the verb _chegar_;
* et cetera.

# __8. Multiple White spaces__
Because we took away punctuation, we ended up with double white spaces in between words
in certain comments. This was no problem for the lemmatization process but it 
might be in the future.\n 

__Example:__
``` {r }
geo_sao_paolo_df$review_comment_message[27]
```
It's visible in the part "excelente  o produto."\n

This is fixed by replacing all < 1 consecutive white spaces with 1 white space. 
``` {r }
geo_sao_paolo_df <- geo_sao_paolo_df %>%
  mutate(
    review_comment_message = str_replace_all(review_comment_message, "\\s+", " ")
        )
```

__Same example after treatment:__
``` {r }
geo_sao_paolo_df$review_comment_message[27]
```


# __9. Unreliability in text data__
## __9.1. Inconsistent application of diacritics in the data__
The current Portuguese language makes use of four different diacritics:

* __Tilde ~__ (Eu não quero nada = I don't want anything)
* __Acute ´__ (é tempo de = it's about time)
* __Circumflex ^__ (Soy Português = I'm Portuguese)
* __Grave `__ (ir à escola = go to school)
* __Cedilla ¸__ (a comunicação entre as pessoas = communication between people)

The data show that not every customer uses diacritics.

__Number of cases that skip the use of a tilde (~) in "não":__ 
``` {r }
print(
 length(
  geo_sao_paolo_df[
   grepl(
    'nao',
    geo_sao_paolo_df$review_comment_message) == TRUE,]$review_comment_message))

head(
 geo_sao_paolo_df[
  grepl(
   'nao',
    geo_sao_paolo_df$review_comment_message) == TRUE,]$review_comment_message)
```

__Number of cases that don't:__
```{r }
print(
 length(
  geo_sao_paolo_df[
    grepl(
     'não',
     geo_sao_paolo_df$review_comment_message) == TRUE,]$review_comment_message))


head(
 geo_sao_paolo_df[
  grepl(
   'não',
   geo_sao_paolo_df$review_comment_message) == TRUE,]$review_comment_message)

```

This example shows that for only one word - albeit an extremely common one that 
will potentially be wiped out by the stopword filtering process - it is possible
to miss out on 482 cases because of diacritic inconsistency.

## __9.2. Solution: Spellchecker and autocorrect__
Before we do anything, you might wonder: "why would you not use a spellchecker 
_before_ the lemmatization process? Long story short: because it doesn't work 
reliably. Unfortunately, this is a very difficult task. The code below uses the `hunspell` dictionary and library to make an attempt at recognizing and correcting spelling mistakes.\n

Firstly, load the library and dictionary. "pt_BR" is the most recent (2017) 
available Portuguese-Brazilian dicitionary that can be used with `hunspell` library.
```{r message = FALSE}
library(hunspell)
dict <- dictionary("pt_BR")
```

Secondly, I set up a sentence that has relatively simple spelling errors:

* "nao" should be não;
* "solicitarr" should be solicitar;
* "ja" should be já.

```{r }
bad <- hunspell(
  "nao entregar o pedir dentro do prazo ja solicitarr cancelamento", 
  dict = dict)

print(bad[[1]])
```

The dictionary manages to spot the spelling errors. But can it give us legitimate
suggestions on what the words should be?

```{r }
hunspell_suggest(bad[[1]])
```
Apparently, it can't. In conclusion, we need something else because the dictionary method is going to 
be too unreliable.

## __9.3. Solution: standardization by altering character encoding__

We can't fix the problem but we can make the situation better from an NLP perspective.
The diacritic-free variant of a word is the common denominator. 
The diacritic inconsistency can hence be fixed by turning all text to ASCII character 
encoding. Yes, we will lose our diacritics but diacritics have a predominantly 
phonetic utility and don't change much in terms of semantics. It's not ideal
but it's the lesser of two evils.

``` {r }
geo_sao_paolo_df <- geo_sao_paolo_df %>%
  mutate(
    # Convert message
    review_comment_message = iconv(
       review_comment_message, 
       to = "ASCII//TRANSLIT"),
    # Convert title
     review_comment_title = iconv(
       review_comment_title,
       to = "ASCII//TRANSLIT")
        )
```



# __10. Stop words & Weird words__

## __10.1. What stop words are and why they matter__
The definition of "stop word" is important because removing stop words is a 
somewhat subjective process. I would describe _stop word_ as "any word that 
does not contribute to learning the semantics of a piece of text." As such, 
using some sort of 'universal stop word list' is not going to get us far.\n

We want to remove stop words because:

* doing so decreases data and thereby training time;
* It removes statistical noise, which can alter outcomes of NLP methods. 

## __10.2. Words in tidy format__

``` {r}
text_df <- tibble(line = 1:nrow(geo_sao_paolo_df), 
                  text = as.character(geo_sao_paolo_df$review_comment_message))

new_text_df <- text_df %>%
  unnest_tokens(word, text) %>%
  count(word, sort = TRUE)

new_text_df[-1,] %>%
  kbl() %>%
  kable_paper(full_width = T) %>%
  scroll_box(height = "500px")
```

## __10.3. Creating a custom stopwords collection__
Using a collection of words we deem as 'stop words', we can clean the data. 

Initialize empty vector.
```{r }
dic <- vector()
```

Add column with info on word length.
``` {r }
new_text_df <- new_text_df %>%
  mutate(
    word_length = nchar(word)
  )
```

Filter out one- and two-letter words as these don't bring any semantic relevance.
``` {r }
# df to capture the words
super_short_words <- new_text_df %>%
  filter(word_length < 3) 

# show the words
super_short_words %>%
  kbl() %>%
  kable_paper(full_width = T) %>%
  scroll_box(height = "500px")

# add to dic
dic <- c(dic, super_short_words$word)

# Moving forward, we can filter these out of the tidy df
new_text_df <- new_text_df %>%
  filter(!word_length < 3) 

```

Filter out words that have more than 2 consecutive characters. Again,
in a perfect world we would _correct_ rather than _filter_. A more sophisticated
solution might be used in a later stadium. For now, this sufficies, especially 
considering that there are relatively few of these cases left after initial
filtering in [chapter 3](#3).
```{r }
new_text_df %>%
  filter(grepl('([a-z\\d])\\1\\1', word)) %>%
  kbl() %>%
  kable_paper(full_width = T) %>%
  scroll_box(height = "500px")

weird_words <- new_text_df %>%
  filter(grepl('([a-z\\d])\\1\\1', word))

# add to dic
dic <- c(dic, weird_words$word)


# Moving forward, we can filter these out of the tidy df
new_text_df <- new_text_df %>%
  filter(!grepl('([a-z\\d])\\1\\1', word))
```

Words with no vowels
```{r }
new_text_df %>%
  filter(!grepl('[aeiou]+', word)) %>%
  kbl() %>%
  kable_paper(full_width = T) %>%
  scroll_box(height = "500px")

no_vowels <- new_text_df %>%
  filter(!grepl('[aeiou]+', word))

dic <- c(dic, no_vowels$word)


new_text_df <- new_text_df %>%
  filter(grepl('[aeiou]+', word))
```

Words with only vowels
```{r }
new_text_df %>%
  filter(!grepl('[^aeiou]+', word)) %>%
  kbl() %>%
  kable_paper(full_width = T) %>%
  scroll_box(height = "500px")

on_vowels <- new_text_df %>%
  filter(!grepl('[aeiou]+', word))

dic <- c(dic, on_vowels$word)

new_text_df <- new_text_df %>%
  filter(grepl('[aeiou]+', word))

```

An additional stopwords list was used with very generic words that are
'save' for our research context.\n

We want to be careful to take out "nao" from our dic because it is a valence 
shifter which may be relevant for lexicon-based sentiment analysis.

```{r }
# external stopwords document tha
port_stopwords <- read.table("stopwords.txt", sep = "", header=F)

# standardize them
port_stopwords <- port_stopwords %>%
  mutate(V1 = iconv(V1, to = "ASCII//TRANSLIT"))

port_stopwords <- port_stopwords %>%
  filter(!grepl('nao', V1))

dic <- c(dic, port_stopwords$V1)
```
Avoid overlap in our dictionary
```{r }
dic <- unique(dic)
```

## __10.4. Filtering topwords with custom dictionary__

```{r eval=FALSE}
empty_df <- as.data.frame(geo_sao_paolo_df$review_id)
empty_df$iterator <- c(1:nrow(empty_df))
empty_df$new_stuff <- 0 

iterator <- 1

for (i in geo_sao_paolo_df$review_comment_message) {
  print(paste(rm_stopwords(i, dic)[[1]], collapse = " "))
  empty_df$new_stuff[iterator] <- paste(rm_stopwords(i, dic)[[1]], collapse = " ")
  iterator = iterator + 1
}
geo_sao_paolo_df$review_comment_message <- empty_df$new_stuff



```

``` {r eval = FALSE, echo=FALSE}
write.csv(empty_df, "stopword_filtered_sp_reviews.csv")
```

```{r echo= FALSE}
empty_df <- read.csv("stopword_filtered_sp_reviews.csv")

empty_df <- empty_df %>%
  select(-X)
```
Replace unfiltered column with filtered comments
``` {r }
geo_sao_paolo_df$review_comment_message <- empty_df$new_stuff
```

# __11. Write data__
For easy use in another script.
```{r }
write.csv(geo_sao_paolo_df,"sp_2_cleaned_text.csv", row.names = FALSE)
```



