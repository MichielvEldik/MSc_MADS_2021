---
title: "External merge and first insights"
author: "Michiel van Eldik"
date: "`r Sys.Date()`"
output:
   rmdformats::downcute:
    self_contained: true
    thumbnails: true
    lightbox: true
    gallery: false
    highlight: tango
---

# __<a href="index.html">Back to main</a>__

<center> <a href="index.html">Back to main</a> </center>


# __1. Load data and libraries__

```{r message = FALSE}
library(lubridate)
library(dplyr)
library(kableExtra)
library(sf)
library(sp)
library(rgdal)
library(spatialEco)
library(revgeo)
library(car)
library(readxl)
library(koRpus)
library(stringr)
```

HDI data
```{r }
my_data <- read_excel("data (3).xlsx")
my_data$Territorialidades <- as.character(my_data$Territorialidades)
my_data$Territorialidades <- iconv(my_data$Territorialidades, to = 'ASCII//TRANSLIT')
my_data$Territorialidades <- tolower(my_data$Territorialidades)
my_data <- my_data[-c(1,2),]
``` 

```{r }
geo_sao_paolo_df <- geo_sao_paolo_df %>%
    mutate(customer_city = paste(customer_city, "(", sep = " "),
           customer_city = paste(customer_city, customer_state, sep = ""),
           customer_city = paste(customer_city, ")", sep=""))

geo_sao_paolo_df$customer_city <- tolower(geo_sao_paolo_df$customer_city)
```

```{r }

merry <- merge(geo_sao_paolo_df,
               my_data,
               by.x = 'customer_city',
               by.y = 'Territorialidades',
               all.x = TRUE)

```

```{r }
yoyo <- glm(message_bool ~ 
                `IDHM 2010` + 
                top2box + 
                max_price +
                y_2018 +
                y_2017 + 
                item_count +
                review_sent_dow +
                product_weight_g + 
                product_category_name
                , data = merry,  family='binomial')
summary(yoyo)

vif(yoyo)


c_north <- c("AC","AP","AM","PA", "RO", "RR", "TO")
c_south <- c("SC", "RS", "PR")
c_southeast <- c("SP", "RJ", "MG", "ES")
c_northeast <- c("AL", "BA", "CE", "MA", "RN", "SE", "PI", "PB", "PE")
c_centerwest <- c("MT", "MS", "GO", "DF")
```

```{r }

merry$north <- 0
merry$south <- 0
merry$southeast <- 0
merry$northeast <- 0
merry$centerwest <- 0


merry <- merry %>% 
    mutate(north = ifelse(customer_state %in% c_north, 1,0),
           south = ifelse(customer_state %in% c_south, 1,0),
           southeast = ifelse(customer_state %in% c_southeast, 1,0),
           northeast = ifelse(customer_state %in% c_northeast, 1,0),
           centerwest = ifelse(customer_state %in% c_centerwest, 1,0),
           )
```


```{r }

yoyo <- glm(message_bool ~ 
                `IDHM 2010` + 
                top2box + 
                max_price +
                y_2018 +
                y_2017 + 
                item_count +
                review_sent_wknd +
                #review_sent_dow +
                product_weight_g + 
                north +
                south  + 
                #southeast + 
                northeast
                , data = merry,  family='binomial')
summary(yoyo)



```
```{r }
vif(yoyo)
```
How well are low HDI reviews represented in the data?
```{r }
sud <- merry[merry$south == 1 ,]
sud <- sud$`IDHM 2010`
hist(sud)
norte <- merry[merry$north ==1 ,]
norte <- norte$`IDHM 2010`
hist(norte)
norteste <- merry[merry$northeast == 1,]
norteste <- norteste$`IDHM 2010`
hist(norteste)
```


```{r }
zhu <- lm(message_length ~
                `IDHM 2010` + 
                top2box + 
                max_price +
                y_2018 +
                y_2017 + 
                item_count +
                review_sent_wknd +
                #review_sent_dow +
                product_weight_g + 
                north +
                south  + 
                #southeast + 
                northeast
                , data = merry[merry$message_length >0,])

summary(zhu)
```
```{r }
vif(zhu)
```

```{r }

gsub("\\s*\\([^\\)]+\\)","",as.character(companies$Name))

my_data <- my_data %>%
    mutate(Territorialidades =  gsub("\\s*\\([^\\)]+\\)","", Territorialidades))
```

```{r }
input <- read.csv("sp_3_derived_vars.csv")

# full variant
input_2 <- read.csv("full_geomerged_df_3.csv")
```

```{r }
geo_sao_paolo_df <- input

geo_sao_paolo_df <- input_2

geo_sao_paulo_df <- geo_sao_paolo_df[geo_sao_paolo_df$customer_state == "AC",]

```


# __2. Connect to shapefile to assign municiaplity to cases__
There are a few NAs for the coordinates. We'll move on without them for now.
```{r }

print(nrow(geo_sao_paolo_df[is.na(geo_sao_paolo_df$centroid_lat),]))

geo_sao_paolo_df <- geo_sao_paolo_df[!is.na(geo_sao_paolo_df$centroid_lat),]

```

Turn them into coordinate system (or something)
```{r }
coordinates(geo_sao_paolo_df) = c('centroid_long', 'centroid_lat')
```


We are using the 'regular' reference system,
```{r }
# assign coordinate system 
crs.geo1 = CRS("+proj=longlat")
proj4string(geo_sao_paolo_df) = crs.geo1
```

```{r }
# Load shapefile 
# state map
sp_map_2 <- readOGR("./shp_files/BR_Localidades_2010.shp")

sp_map_1 <- readOGR('sao_paulo.shp')

ac_map <- readOGR("./shp_files/ac_munic/12MUE250GC_SIR.shp")

e <- over(ac_map, geo_sao_paolo_df) 

```

```{r }
plot(sp_map_2)
points(geo_sao_paolo_df)
```




```{r }
# Connect shapefile polygon data on municipality classification with our point data
new_guy <- point.in.poly(geo_sao_paolo_df, ac_map, sp = FALSE)

# transform it back to a normal dataframe in order to do further merging and analyses
new_guy_df <- as.data.frame(new_guy)

```




```{r }
nieuw<- as.data.frame(sp_map_2)
```


Return nearest neighbor index of other dataframe
```{r }
library(FNN)
```

```{r }
nn1 = get.knnx(coordinates(sp_map_2), coordinates(geo_sao_paolo_df), 1)

ii = nn1$nn.index[,1]
ii

new_guy_df <- as.data.frame(geo_sao_paolo_df)
new_guy_2 <- as.data.frame(sp_map_2)
```


All data points distributed throughout the municipalities of Sao Paulo state.
```{r }
plot(sp_map_2)
points(geo_sao_paolo_df)
```

Municipality name connected to original data
```{r }
# Connect shapefile polygon data on municipality classification with our point data
new_guy <- point.in.poly(geo_sao_paolo_df, sp_map_2)

# transform it back to a normal dataframe in order to do further merging and analyses
new_guy_df <- as.data.frame(new_guy)

```


# __3. Incorporate municipality-level HDI__
```{r }
muni_hdi_sp_state <- read.csv('sp_state_municipalities_hdi.csv', header = FALSE)
```

Standardize data for merge
```{r }
# standardize words with weird Portuguese characters and accents
muni_hdi_sp_state$V1 <- iconv(muni_hdi_sp_state$V1, to = 'ASCII//TRANSLIT')
# to lower case
muni_hdi_sp_state$V1 <- tolower(muni_hdi_sp_state$V1)
```
Same for other data. 
```{r }
# Unfortunately, there are still a bunch of NAs 
new_guy_df$NOMEMUNICP <- iconv(new_guy_df$NOMEMUNICP, to = 'ASCII//TRANSLIT')
new_guy_df$NOMEMUNICP <- tolower(new_guy_df$NOMEMUNICP)
```

Merge! 
```{r }
merry <- merge(new_guy_df,
               muni_hdi_sp_state,
               by.x = 'NOMEMUNICP',
               by.y = 'V1',
               all.x = TRUE)
```

# __4. First analyses__

## __4.1. Comparing HDI and commenting behaviour__

```{r }
bmod <- glm(title_or_message ~ log(V2), na.action=na.exclude, data = merry[merry$customer_city != 'sao paulo',], family = 'binomial')
print(summary(bmod))
```

Interestingly there is a negative correlation between creation of reviews and 
human development index. That means a lower HDI leads to a higher probability of
creating a review. There can be many explanations for this:

* Lower HDIs are prone to delays, causing more tardiness and more reviews. 
* Expected delivery times are calculated with more 'space' because delivery times
on these routes are more volatiles. In that case, there might be a lot of early 
deliveries, causing people to be happily surprised and commenting.
* HDI is related to product category, which in turn can be related to comment 
propensity. 


__Interestingly__ mean number of words is positively related to HDI if we take
the non-respondents out of the equation. 


```{r }
lmod <- lm(log(bef_nchar) ~ log(V2), 
           na.action=na.exclude, 
           data = merry[merry$bef_nchar > 0,])
print(summary(lmod))
```

Same for number of words.
```{r }
lmod <- lm(log(bef_nwords) ~ log(V2), 
           na.action=na.exclude, 
           data = merry[merry$bef_nchar > 0,])
print(summary(lmod))
```

No evidence that HDI is related to average word length.
```{r }
lmod <- lm(log(nchar_perword) ~ log(V2), 
           na.action=na.exclude, 
           data = merry[merry$bef_nchar > 0,])
print(summary(lmod))
```

## __4.2. Other variables affecting review creation__
__Sao Paulo__ is excluded as it is a huge city that should be looked at in more detail.
```{r }
bmod <- glm(message_bool ~ log(V2) + item_count + max_price + review_sent_wknd + review_score + log(diff_est_deliv + 176),
            na.action=na.exclude, 
            data = merry[merry$customer_city != 'sao paulo',], family = 'binomial')
print(summary(bmod))
print(vif(bmod))
```


Some interesting results already! 

* Negative reviews make for higher creation probability
* Positive correlation between diff_est_deliv and review probability because
a higher value for this variable implies there was a large difference between
estimated delivery time and actual delivery time. I.e., being too late (too early 
are indicated with negative)
* V2 signifies HDI. Lower HDI means higher probability of review creation for 
some reason I don't know yet. It's opposite of what I expected.
* Higher prices and item counts elicite review creation.


Takeaways:

* The weird outcome for relationship between review creation and HDI can be due
to the __age__ factor. because younger people are more inclined to use the 
internet and low-HDI areas tend to have a lower age pyramid distribution. 
* There are many deliveries that were reported in the comments as too late, but
that are indicated in delivery data as on-time. It's important to deal with this.
* In general, it's important to be able to label what the topic of conversation is.


I plan to do this by building a text classifier that classifies whether a comment
is frieght related by basing it on data that are confirmed to be delayed.\n

Moving on... Let's have a look at product categories.

```{r }
bmod <- glm(message_bool ~ product_category_name,
            na.action=na.exclude, 
            data = merry[merry$customer_city != 'sao paulo',], family = 'binomial')
print(summary(bmod))
```

There is too muchgoing on to really say anything at this point. 
The data need to be classified into categorizations and then we can actually derive 
meaning from this. However, what is notable:

* __construction_tools_safety__ is a relatively small (n = 56) category but 
seems to evoke comments quite strongly. 
* __home_comfort__ also is not a huge (n=174) category, but has a strong effect.
* Same story for __male clothing__ that has only n = 48.


## __4.3. Other variables affecting review length__

```{r }
lmod <- lm(log(bef_nchar) ~ log(V2) + diff_rev_crea_ans + review_sent_wknd + max_price + review_answer_wknd + diff_est_deliv + top2box, 
           na.action=na.exclude, 
           data = merry[merry$bef_nchar > 0,])
print(summary(lmod))
print(vif(lmod))
```

Takeaways... 

* Lower review scores elecit longer messages, accounting for the fact that "0" 
length messages are not in the equation.
* Higher priced products elicit longer messages.
* Higher HDI elicits longer messages. 

## __4.4. Lexical diversity__

AS fun little instrument in the `koRpus` package is `textstat_lexdiv`. Let
's se how it works. It requires longer texts to run. So maybe a future project whereby I collect all comments from a certain HDI range or review 
score range. It's good to know that this is availables.

```{r eval=FALSE }

library(koRpus.lang.en)
example_1 <- "hey jude"
example_2 <- "hey jude, dont make it bad"

available.koRpus.lang()

lex.div(example_1, force.lang= "en")
```


# __5. Moving forward__

There is still a lot of work to do, especially in the area of categorizing comments; 
factoring things like 'age' into the equation; looking at other states; comparing 
urbanity; looking at neighborhood level; classifying products; etc.\n

What I'm thinking about the most is the negative statistical relationship 
between HDI and revieww creation and the positive relationship between HDI and 
review length. The __age distribution__ account is something to explore. Moreover,
this is an extremely limited analysis because it assumes one level of data 
whilst in actuality, we have nested data because we are using municipal-level 
data and individual-level data.\n

Be that as it may, we have a nice beginning. Even for a state like Sao Paulo, 
which has mostly municipalities on the higher end of the HDI spectrum, we can 
see that there is indeed enough variance to get to some interesting insights.

